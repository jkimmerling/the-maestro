# Codex Tool Specifications

This document outlines the tool specifications for the Codex agent. It covers the structure of the tools, how they are called, and the differences between authentication methods.

## 1. Overview

Codex uses a two-tiered approach to tools. Instead of exposing a large number of individual tools to the LLM, it exposes two main "meta-tools":

*   `codex`: Starts a new Codex session.
*   `codex-reply`: Continues an existing Codex session.

The LLM calls these meta-tools, and the `mcp-server` then internally decides which of the "real" tools to make available to the agent based on the configuration provided in the meta-tool call.

## 2. Meta-Tools

### 2.1. `codex`

This tool is used to start a new Codex session.

**JSON Schema:**

```json
{
  "name": "codex",
  "title": "Codex",
  "description": "Run a Codex session. Accepts configuration parameters matching the Codex Config struct.",
  "inputSchema": {
    "type": "object",
    "properties": {
      "approval-policy": {
        "description": "Approval policy for shell commands generated by the model: `untrusted`, `on-failure`, `on-request`, `never`.",
        "enum": [
          "untrusted",
          "on-failure",
          "on-request",
          "never"
        ],
        "type": "string"
      },
      "sandbox": {
        "description": "Sandbox mode: `read-only`, `workspace-write`, or `danger-full-access`.",
        "enum": [
          "read-only",
          "workspace-write",
          "danger-full-access"
        ],
        "type": "string"
      },
      "config": {
        "description": "Individual config settings that will override what is in CODEX_HOME/config.toml.",
        "additionalProperties": true,
        "type": "object"
      },
      "cwd": {
        "description": "Working directory for the session. If relative, it is resolved against the server process's current working directory.",
        "type": "string"
      },
      "include-plan-tool": {
        "description": "Whether to include the plan tool in the conversation.",
        "type": "boolean"
      },
      "model": {
        "description": "Optional override for the model name (e.g. \"o3\", \"o4-mini\").",
        "type": "string"
      },
      "profile": {
        "description": "Configuration profile from config.toml to specify default options.",
        "type": "string"
      },
      "prompt": {
        "description": "The *initial user prompt* to start the Codex conversation.",
        "type": "string"
      },
      "base-instructions": {
        "description": "The set of instructions to use instead of the default ones.",
        "type": "string"
      }
    },
    "required": [
      "prompt"
    ]
  }
}
```

### 2.2. `codex-reply`

This tool is used to continue an existing Codex session.

**JSON Schema:**

```json
{
  "description": "Continue a Codex session by providing the session id and prompt.",
  "inputSchema": {
    "properties": {
      "prompt": {
        "description": "The *next user prompt* to continue the Codex conversation.",
        "type": "string"
      },
      "sessionId": {
        "description": "The *session id* for this conversation.",
        "type": "string"
      }
    },
    "required": [
      "prompt",
      "sessionId"
    ],
    "type": "object"
  },
  "name": "codex-reply",
  "title": "Codex Reply"
}
```

## 3. "Real" Tools

The following tools are not directly exposed to the LLM, but are made available to the agent internally based on the configuration provided in the `codex` meta-tool call.

### 3.1. `shell`

*   **Description**: Runs a shell command and returns its output.
*   **Enabled by**: Always enabled.
*   **Source File**: `codex-rs/core/src/codex.rs` (handled by `handle_container_exec_with_params`)
*   **Parameters**:
    *   `command` (array of strings, required): The command to execute.
    *   `workdir` (string, optional): The working directory to execute the command in.
    *   `timeout_ms` (number, optional): The timeout for the command in milliseconds.
    *   `with_escalated_permissions` (boolean, optional): Whether to request escalated permissions.
    *   `justification` (string, optional): A justification for running with escalated permissions.

*   **Flow**:
    1.  **Tool Availability**: The `shell` tool is always available to the LLM.
    2.  **LLM Call**: The LLM generates a JSON object with the tool name and parameters.
        ```json
        {
          "name": "shell",
          "arguments": "{\"command\":[\"ls\",\"-l\"]}"
        }
        ```
    3.  **Tool Execution**: The `handle_container_exec_with_params` function in `codex.rs` is called. It executes the command in a sandboxed environment and captures the output.
    4.  **Return to LLM**: The tool returns a `ResponseInputItem::FunctionCallOutput` with the `stdout`, `stderr`, and `exit_code` of the command, formatted as a JSON string.
        ```json
        {
          "call_id": "...",
          "output": {
            "content": "{\"output\":\"...\",\"metadata\":{\"exit_code\":0,\"duration_seconds\":0.1}}",
            "success": true
          }
        }
        ```

### 3.2. `update_plan`

*   **Description**: Updates the task plan.
*   **Enabled by**: The `include_plan_tool` boolean flag in the `codex` tool call.
*   **Source File**: `codex-rs/core/src/plan_tool.rs`
*   **Parameters**:
    *   `explanation` (string, optional): An explanation of the plan.
    *   `plan` (array of objects, required): A list of plan items, where each item has:
        *   `step` (string, required): The description of the step.
        *   `status` (string, required): The status of the step, one of "pending", "in_progress", or "completed".

*   **Flow**:
    1.  **Tool Availability**: The `update_plan` tool is available if `include_plan_tool` is true.
    2.  **LLM Call**: The LLM generates a JSON object with the tool name and parameters.
        ```json
        {
          "name": "update_plan",
          "arguments": "{\"plan\":[{\"step\":\"First step\",\"status\":\"completed\"},{\"step\":\"Second step\",\"status\":\"in_progress\"}]}"
        }
        ```
    3.  **Tool Execution**: The `handle_update_plan` function in `plan_tool.rs` is called. It sends a `PlanUpdate` event to the client.
    4.  **Return to LLM**: The tool returns a `ResponseInputItem::FunctionCallOutput` with the content `"Plan updated"`.

### 3.3. `apply_patch`

*   **Description**: Use the `apply_patch` tool to edit files.
*   **Enabled by**: The `include_apply_patch_tool` boolean flag in the `codex` tool call.
*   **Source Files**: `codex-rs/core/src/tool_apply_patch.rs`, `codex-rs/core/src/codex.rs` (handled by `handle_container_exec_with_params`)
*   **Two versions**:
    1.  **Freeform**: Uses a custom Lark grammar for the patch format.
    2.  **JSON**: Takes a single `input` string parameter containing the patch in a custom format.
*   **Patch Format**: A custom, file-oriented diff format that supports adding, deleting, and updating files.

*   **Flow**:
    1.  **Tool Availability**: The `apply_patch` tool is available if `include_apply_patch_tool` is true.
    2.  **LLM Call**: The LLM generates a JSON object with the tool name and parameters.
        ```json
        {
          "name": "apply_patch",
          "arguments": "{\"input\":\"*** Begin Patch\\n*** Update File: README.md\\n@@ -1,1 +1,1 @@\\n-Hello\\n+Hello, world!\\n*** End Patch\\n\"}"
        }
        ```
    3.  **Tool Execution**: The `handle_container_exec_with_params` function in `codex.rs` is called. It applies the patch to the specified file.
    4.  **Return to LLM**: The tool returns a `ResponseInputItem::FunctionCallOutput` with the result of the patch application.

### 3.4. `web_search`

*   **Description**: Performs a web search.
*   **Enabled by**: The `tools_web_search_request` boolean flag in the `codex` tool call.
*   **Source File**: `codex-rs/core/src/codex.rs`
*   **Parameters**: None.

*   **Flow**:
    1.  **Tool Availability**: The `web_search` tool is available if `tools_web_search_request` is true.
    2.  **LLM Call**: The LLM generates a `web_search` tool call.
    3.  **Tool Execution**: The `handle_response_item` function in `codex.rs` sends a `WebSearchEnd` event to the client.
    4.  **Return to LLM**: This tool does not return a value to the LLM.

### 3.5. `view_image`

*   **Description**: Attach a local image to the conversation context.
*   **Enabled by**: The `include_view_image_tool` boolean flag in the `codex` tool call.
*   **Source File**: `codex-rs/core/src/codex.rs`
*   **Parameters**:
    *   `path` (string, required): The local filesystem path to the image file.

*   **Flow**:
    1.  **Tool Availability**: The `view_image` tool is available if `include_view_image_tool` is true.
    2.  **LLM Call**: The LLM generates a JSON object with the tool name and parameters.
        ```json
        {
          "name": "view_image",
          "arguments": "{\"path\":\"/path/to/image.png\"}"
        }
        ```
    3.  **Tool Execution**: The `handle_function_call` function in `codex.rs` is called. It creates an `InputItem::LocalImage` with the absolute path to the image and injects it into the current task's pending input.
    4.  **Image Processing**: In the next turn, the `InputItem::LocalImage` is processed. The `mcp-server` reads the image file, encodes it as a Base64 data URL, and includes it in the `ContentItem::Image` of the message sent to the LLM.
    5.  **Return to LLM**: The `view_image` tool itself returns a `ResponseInputItem::FunctionCallOutput` with a confirmation message, e.g., `"attached local image path"`. The actual image data is sent in the next message to the LLM.

## 4. Tool Message Format

The format of the tool messages sent to the LLM depends on the authentication method being used.

### 4.1. Personal ChatGPT Account (OAuth) - Responses API

When using a personal ChatGPT account, the tools are sent using the "Responses API" format. The tools are sent as a JSON array in the `tools` field of the request.

**Example Tool Call:**

```json
{
  "type": "function",
  "name": "shell",
  "description": "Runs a shell command and returns its output",
  "strict": false,
  "parameters": {
    "type": "object",
    "properties": {
      "command": {
        "type": "array",
        "items": {
          "type": "string"
        },
        "description": "The command to execute"
      },
      "workdir": {
        "type": "string",
        "description": "The working directory to execute the command in"
      },
      "timeout_ms": {
        "type": "number",
        "description": "The timeout for the command in milliseconds"
      }
    },
    "required": [
      "command"
    ],
    "additionalProperties": false
  }
}
```

### 4.2. API Key - Chat Completions API

When using an API key, the tools are sent using the "Chat Completions API" format. The tools are sent as a JSON array in the `tools` field of the request, where each tool is wrapped in a `{"type": "function", "function": ...}` object.

**Example Tool Call:**

```json
{
  "type": "function",
  "function": {
    "name": "shell",
    "description": "Runs a shell command and returns its output",
    "strict": false,
    "parameters": {
      "type": "object",
      "properties": {
        "command": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "The command to execute"
        },
        "workdir": {
          "type": "string",
          "description": "The working directory to execute the command in"
        },
        "timeout_ms": {
          "type": "number",
          "description": "The timeout for the command in milliseconds"
        }
      },
      "required": [
        "command"
      ],
      "additionalProperties": false
    }
  }
}
```

## 5. System Prompt

The system prompt can be set using the `base_instructions` parameter in the `codex` tool call. This allows for customizing the agent's behavior for different tasks. The system prompt can be changed for each turn by providing a new `base_instructions` value.

## 6. Authentication

Codex supports two authentication methods:

*   **Personal ChatGPT Account (OAuth)**: This method uses an OAuth flow to get an access token. The tool calls are made to the internal "Responses API".
*   **API Key**: This method uses an OpenAI API key. The tool calls are made to the public "Chat Completions API".

The authentication method affects the format of the tool messages, as described above. The core logic for handling tools is the same for both authentication methods.

```