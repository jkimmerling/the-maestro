# Story 1.6: Epic 1 Demo Creation

**Demonstrating End-to-End System Integration with Runnable Examples**

## Overview

Story 1.6 represents the culmination of Epic 1's foundational work by creating a comprehensive, runnable demo that showcases all the core capabilities implemented so far. This tutorial teaches you how to create effective demos for complex Elixir/OTP applications and demonstrates the complete integration of The Maestro's core agent engine.

## Learning Objectives

By the end of this tutorial, you will understand how to:

- Create runnable demos for OTP applications with external dependencies
- Test complete system integration including LLM providers and tooling
- Handle errors gracefully in demo environments  
- Document complex system requirements and troubleshooting
- Demonstrate architecture patterns through practical examples

## What We Built in Epic 1

Before diving into the demo creation, let's review what Epic 1 accomplished:

### ðŸ—ï¸ **Foundation Architecture**
- **OTP Application**: Fault-tolerant supervision tree with dynamic process management
- **Agent GenServer**: Stateful conversation management with complete message history
- **Registry System**: Process discovery and lifecycle management for multiple concurrent agents

### ðŸ¤– **LLM Integration**
- **Provider Pattern**: Model-agnostic `LLMProvider` behaviour with concrete Gemini implementation
- **Authentication Layer**: Support for API Key, OAuth2, and Service Account authentication
- **Conversation Flow**: Complete request-response cycle with context preservation and error recovery

### ðŸ› ï¸ **Tooling System**
- **Security Framework**: Sandboxed file operations with path validation and directory restrictions
- **Tool Registry**: Dynamic tool registration and discovery system
- **Function Calling**: Deep integration with LLM provider function calling capabilities

## Demo Architecture Design

Creating an effective demo for a complex system like The Maestro requires careful architectural consideration:

### Demo Requirements Analysis

```elixir
# Demo must demonstrate:
# 1. Application startup and supervision tree initialization
# 2. Agent process lifecycle (spawn, configure, communicate, terminate)
# 3. LLM provider integration with real API calls
# 4. Tool system functionality with file operations
# 5. Error handling and recovery patterns
# 6. State management and process supervision
```

### Design Patterns Used

#### 1. **Self-Contained Execution**
The demo script is designed to be completely self-contained and runnable with minimal setup:

```elixir
defmodule Epic1Demo do
  def run do
    case ensure_application_started() do
      :ok -> run_demo_conversation()
      {:error, reason} -> exit_with_instructions()
    end
  end
  
  defp ensure_application_started do
    # Check if already running (mix run context)
    if application_running?(:the_maestro) do
      :ok
    else
      Application.ensure_all_started(:the_maestro)
    end
  end
end
```

#### 2. **Progressive Demonstration**
The demo follows a structured sequence that builds complexity:

```elixir
defp demo_conversation_sequence(agent_id) do
  # Test 1: Basic LLM integration
  test_simple_conversation(agent_id)
  
  # Test 2: Tool-assisted capabilities  
  test_file_tool_usage(agent_id)
  
  # Final: State inspection
  display_final_state(agent_id)
end
```

#### 3. **Comprehensive Error Handling**
Each demo step includes specific error handling with actionable troubleshooting:

```elixir
defp test_simple_conversation(agent_id) do
  case TheMaestro.Agents.send_message(agent_id, message) do
    {:ok, response} ->
      display_success(response)
    {:error, reason} ->
      suggest_auth_troubleshooting()
  end
end
```

## Implementation Deep Dive

### Step 1: Application Context Management

The most critical aspect of demo creation for OTP applications is proper application context management:

```elixir
defp ensure_application_started do
  # Check if application is already running (when run with mix run)
  if Application.started_applications() 
     |> Enum.any?(fn {app, _, _} -> app == :the_maestro end) do
    IO.puts("Application already running - using existing instance")
    :ok
  else
    # Try to start the application and all dependencies
    case Application.ensure_all_started(:the_maestro) do
      {:ok, _apps} -> 
        # Wait for all supervisors to fully initialize
        Process.sleep(1000)
        :ok
      {:error, reason} -> 
        {:error, reason}
    end
  end
end
```

**Key Design Decisions:**

- **Context Detection**: The demo detects whether it's running in a `mix run` context (application already started) or standalone
- **Dependency Management**: Uses `Application.ensure_all_started/1` to handle the entire dependency tree
- **Initialization Timing**: Includes a brief sleep to allow supervisors to fully initialize before proceeding

### Step 2: Agent Lifecycle Demonstration

The demo showcases the complete agent lifecycle with realistic usage patterns:

```elixir
defp run_demo_conversation do
  # Generate unique agent ID for this demo session
  agent_id = "epic1_demo_#{System.system_time(:second)}"
  
  # Start agent with Gemini provider and auto-initialized auth
  case TheMaestro.Agents.start_agent(agent_id, [
    llm_provider: TheMaestro.Providers.Gemini,
    auth_context: nil  # Let agent initialize its own auth
  ]) do
    {:ok, _pid} ->
      demo_conversation_sequence(agent_id)
    {:error, reason} ->
      handle_startup_failure(reason)
  end
end
```

**Implementation Highlights:**

- **Unique IDs**: Uses timestamp-based IDs to prevent conflicts in repeated demo runs
- **Provider Configuration**: Demonstrates explicit provider selection while allowing auth auto-initialization
- **Error Propagation**: Proper error handling with specific troubleshooting guidance

### Step 3: LLM Integration Testing

The demo tests the complete LLM integration chain with realistic prompts:

```elixir
defp test_simple_conversation(agent_id) do
  IO.puts("\nðŸ”¬ TEST 1: Simple LLM Conversation")
  IO.puts("Sending message: 'Hello! Please introduce yourself briefly.'")
  
  case TheMaestro.Agents.send_message(agent_id, "Hello! Please introduce yourself briefly.") do
    {:ok, response} ->
      IO.puts("âœ… LLM Response received!")
      # Display truncated response for demo clarity
      IO.puts("ðŸ’¬ Agent: #{String.slice(response.content, 0, 200)}...")
      
    {:error, reason} ->
      IO.puts("âŒ LLM call failed: #{inspect(reason)}")
      suggest_auth_troubleshooting()
  end
end
```

**Testing Strategy:**

- **Clear Output**: Each test phase is clearly marked and explained
- **Response Handling**: Demonstrates both success and error paths
- **User Experience**: Responses are truncated for demo readability while preserving essential content

### Step 4: Tool System Integration

The file tool demonstration shows the complete tool execution pipeline:

```elixir
defp test_file_tool_usage(agent_id) do
  # Create test file path relative to demo directory
  demo_dir = Path.dirname(__ENV__.file)
  test_file_path = Path.join(demo_dir, "test_file.txt")
  
  message = """
  Please read the contents of the file located at: #{test_file_path}
  
  Use your file reading tool to access this file and tell me what it contains.
  """
  
  case TheMaestro.Agents.send_message(agent_id, message) do
    {:ok, response} ->
      # Check for successful tool execution indicators
      if tool_execution_successful?(response.content) do
        IO.puts("ðŸŽ¯ File tool execution appears successful!")
      else
        suggest_file_tool_troubleshooting()
      end
      
    {:error, reason} ->
      suggest_file_tool_troubleshooting()
  end
end
```

**Tool Testing Approach:**

- **Environment Context**: Uses `__ENV__.file` to determine the correct relative path
- **Realistic Prompts**: Uses natural language that would trigger tool usage
- **Execution Validation**: Parses response content to determine if tools were actually used
- **Path Safety**: Demonstrates secure file operations within allowed directories

## Configuration and Environment Management

### File System Security Configuration

The demo requires specific configuration to enable secure file operations:

```elixir
# config/dev.exs
config :the_maestro, :file_system_tool,
  allowed_directories: [
    "/tmp",
    System.tmp_dir!(),
    Path.join([File.cwd!(), "demos"]),
    Path.join([File.cwd!(), "test", "fixtures"]),
    Path.join([File.cwd!(), "priv"])
  ],
  max_file_size: 10 * 1024 * 1024  # 10MB
```

**Security Considerations:**

- **Directory Restrictions**: Only allows access to safe, predetermined directories
- **File Size Limits**: Prevents abuse through large file operations
- **Path Validation**: All paths are validated and normalized before access

### Authentication Configuration Options

The demo supports multiple authentication methods with clear fallback strategies:

```elixir
# Environment variable options:
# Option 1 - API Key (simplest for demos)
export GEMINI_API_KEY="your-gemini-api-key-here"

# Option 2 - Service Account (enterprise)
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account.json"

# Option 3 - OAuth (interactive, handled automatically)
# OAuth uses browser-based flow with cached credentials at ~/.maestro/oauth_creds.json
# Automatically sets up Code Assist API user during authentication
```

**OAuth Implementation Details:**
- **Dual API Support**: OAuth uses Google Cloud Code Assist API (`cloudcode-pa.googleapis.com`), API keys use Generative Language API
- **Automatic User Setup**: Complete Code Assist user onboarding with proper project configuration
- **Phoenix Integration**: OAuth callback handled by Phoenix server at `/oauth2callback`
- **Credential Caching**: Secure credential storage with automatic token refresh
- **Browser Integration**: Automatic browser opening for seamless user experience

## Error Handling and User Experience

### Comprehensive Error Recovery

The demo implements sophisticated error handling that provides actionable feedback:

```elixir
defp suggest_auth_troubleshooting do
  IO.puts("""
  
  ðŸ”§ AUTHENTICATION TROUBLESHOOTING:
  
  If the LLM calls are failing, ensure you have authentication configured:
  
  Option 1 - API Key:
    export GEMINI_API_KEY="your-gemini-api-key-here"
    
  Option 2 - OAuth (requires browser):
    # Run the application and follow the OAuth flow
    
  Option 3 - Service Account:
    export GOOGLE_APPLICATION_CREDENTIALS="/path/to/service-account.json"
  
  For detailed setup instructions, see demos/epic1/README.md
  """)
end
```

### User Experience Design

The demo prioritizes clear, actionable feedback:

- **Visual Hierarchy**: Uses emojis and formatting to create scannable output
- **Progressive Disclosure**: Shows high-level results first, details on demand
- **Error Context**: Links errors to specific troubleshooting steps
- **Success Indicators**: Clear confirmation when operations succeed

## Testing Strategies for Complex Demos

### Environment Isolation

```elixir
# The demo creates isolated test environments:
agent_id = "epic1_demo_#{System.system_time(:second)}"

# This ensures:
# 1. No conflicts with existing agents
# 2. Unique test sessions
# 3. Clean state for each run
```

### Dependency Validation

```elixir
defp ensure_application_started do
  # Validates entire dependency chain
  case Application.ensure_all_started(:the_maestro) do
    {:ok, apps} ->
      IO.puts("âœ… Started applications: #{inspect(apps)}")
      :ok
    {:error, {app, reason}} ->
      IO.puts("âŒ Failed to start #{app}: #{inspect(reason)}")
      {:error, reason}
  end
end
```

### Integration Testing Approach

The demo serves as a comprehensive integration test:

1. **Application Layer**: OTP supervision tree and process management
2. **Business Logic**: Agent state management and conversation flow
3. **External Services**: LLM provider API integration with authentication
4. **Security Layer**: File system sandboxing and path validation
5. **Error Handling**: Graceful degradation and recovery patterns

## Critical Bug Fixes During Implementation

During the development of this demo, we discovered and fixed several critical bugs in the Gemini provider integration. These fixes highlight the value of comprehensive end-to-end testing:

### OAuth Code Assist API Format Bug

**Issue**: The OAuth implementation was getting 404 "Requested entity was not found" errors despite successful authentication setup. Analysis of the original gemini-cli API calls revealed format mismatches in our Code Assist API requests.

```elixir
# Before (broken) - causing 404 errors:
%{
  model: "models/#{model}",           # Incorrect model prefix
  user_prompt_id: generate_id(),      # Unnecessary field
  request: %{
    generationConfig: %{
      temperature: 0.7,
      maxOutputTokens: 8192           # Wrong parameter name
    }
  }
}

# After (fixed) - matching original gemini-cli:
%{
  model: model,                       # No "models/" prefix
  project: project_id,
  request: %{
    generationConfig: %{
      temperature: 0.0,               # Match original default
      topP: 1                         # Correct parameter
    }
  }
}
```

**Additional fixes required:**
- **Model Name Consistency**: Updated agent.ex to use `"gemini-2.5-pro"` instead of `"gemini-1.5-pro"`
- **HTTP Headers**: Added missing headers (`user-agent`, `x-goog-api-client`, `accept`) to match original
- **Request Structure**: Removed unnecessary `user_prompt_id` field not used by original

**Impact**: OAuth authentication was working correctly, but all API calls were failing with 404 errors until request format was corrected.

### Tool Call Format Bug

**Issue**: The `extract_tool_calls` function was returning maps with atom keys (`:name`, `:arguments`), but the `execute_tool_call` function expected string keys (`"name"`, `"arguments"`).

```elixir
# Before (broken):
%{
  name: call["name"],           # atom key
  arguments: call["args"] || %{}  # atom key
}

# After (fixed):
%{
  "name" => call["name"],        # string key
  "arguments" => call["args"] || %{}  # string key
}
```

**Impact**: Tool calls were failing with `Invalid tool call format` errors, preventing the ReAct loop from completing.

### Message Role Handling Bug  

**Issue**: The `convert_messages_to_gemini` function didn't handle `:tool` role messages, causing `CaseClauseError` when processing tool results.

```elixir
# Before (broken):
case message.role do
  :user -> "user"
  :assistant -> "model"
  :system -> "user"
  # :tool -> ??? (unhandled, causing crash)
end

# After (fixed):
case message.role do
  :user -> "user"
  :assistant -> "model"
  :system -> "user"
  :tool -> "model"  # Tool results treated as model responses
end
```

**Impact**: Agent processes were crashing when trying to send tool results back to the LLM for follow-up responses.

### Learning Points

These bugs demonstrate important principles:

1. **End-to-End Testing**: Unit tests passed, but integration revealed format mismatches between our implementation and the actual API
2. **API Compatibility**: When integrating with existing systems (like gemini-cli), exact format matching is critical - small differences cause complete failures
3. **External API Constraints**: Google's Code Assist API has specific requirements that differ from documentation and require reverse engineering from working implementations
4. **Data Format Consistency**: String vs atom keys must be consistent across the entire pipeline
5. **Error Propagation**: Well-designed error handling helped identify the exact failure points
6. **Authentication vs. Authorization**: OAuth can succeed (authentication) while API calls fail (authorization) due to format issues
7. **Network Analysis**: Capturing actual API calls from working implementations (like gemini-cli) is invaluable for debugging integration issues

## Real-World Demo Output Analysis

Let's analyze the actual demo output to understand what it reveals about the system:

```
âœ… Application started successfully!
ðŸš€ Creating agent with ID: epic1_demo_1754994352
ðŸ” Initializing authentication...
[info] Setting up Code Assist user...
âœ… Authentication initialized successfully!
âœ… Agent started successfully!

ðŸ”¬ TEST 1: Simple LLM Conversation
Sending message: 'Hello! Please introduce yourself briefly.'
[debug] Using project ID: "even-setup-7wxx5"
[debug] Making request to: https://cloudcode-pa.googleapis.com/v1internal:generateContent
âœ… LLM Response received!
ðŸ’¬ Agent: I am Gemini, a large language model built by Google. I'm here to help you with your requests...

ðŸ“Š FINAL AGENT STATE
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Agent ID: epic1_demo_1754994352
Loop State: idle
Message History: 4 messages
LLM Provider: TheMaestro.Providers.Gemini
Auth Status: âœ… Configured
Created At: 2025-08-12 10:24:36.000506Z
```

**What This Output Tells Us:**

- **OAuth Flow Success**: Complete OAuth authentication and Code Assist user setup completed successfully
- **Project Configuration**: Successfully obtained and used Google Cloud project ID (`"even-setup-7wxx5"`)
- **API Integration**: Successful connection to Code Assist API endpoint (`cloudcode-pa.googleapis.com`)
- **Process Management**: Agent was successfully created and supervised
- **State Consistency**: Agent maintained proper state throughout conversation
- **Authentication Success**: Provider was properly initialized with OAuth credentials
- **Message History**: Conversation context was preserved across interactions
- **API Response**: Receiving actual Gemini LLM responses, confirming end-to-end functionality
- **Timestamp Accuracy**: Proper UTC timestamp handling for audit trails

### Successful OAuth Integration Evidence

The debug output shows the complete OAuth integration working:

```
[debug] Request body: {
  "request": {
    "contents": [{"role": "user", "parts": [{"text": "Hello! Please introduce yourself briefly."}]}],
    "generationConfig": {"temperature": 0.0, "topP": 1}
  },
  "project": "even-setup-7wxx5",
  "model": "gemini-2.5-pro"
}
[debug] Headers: [{"authorization", "Bearer ya29.a0AS3H6N..."}, {"content-type", "application/json"}, ...]
```

**Technical Validation:**

- **Request Format**: Matches original gemini-cli exactly (no `user_prompt_id`, correct `generationConfig`)
- **Authentication**: Valid Bearer token in headers
- **Model Selection**: Correct model name (`gemini-2.5-pro`) without invalid prefix
- **Project Context**: Proper Google Cloud project ID integration
- **API Response**: Successful HTTP 200 responses with valid Gemini content

## Best Practices for Demo Creation

### 1. **Comprehensive Documentation**

Every demo should include:
- Prerequisites and setup instructions
- Expected vs. actual output comparison
- Troubleshooting guide for common issues
- Links to related architectural documentation

### 2. **Self-Contained Execution**

Demos should:
- Handle their own application context
- Include all necessary test data
- Provide clear success/failure indicators
- Exit gracefully in all scenarios

### 3. **Educational Design**

Structure demos to:
- Build complexity progressively
- Explain what each step demonstrates
- Show both success and error paths
- Connect to architectural principles

### 4. **Production Readiness**

Even demos should demonstrate:
- Proper error handling patterns
- Security considerations
- Performance characteristics
- Monitoring and observability

## Extension Opportunities

The Epic 1 demo creates a foundation for more advanced demonstrations:

### Multi-Agent Scenarios
```elixir
# Future demos could showcase:
defp test_multi_agent_coordination do
  agents = Enum.map(1..3, fn i -> 
    start_agent("demo_agent_#{i}")
  end)
  # Demonstrate agent coordination patterns
end
```

### Tool Composition
```elixir
# Advanced tool usage patterns:
defp test_complex_tool_usage(agent_id) do
  message = """
  Please read the configuration file and then use that information 
  to create a new file with processed data.
  """
  # Demonstrates multi-step tool usage
end
```

### Performance Testing
```elixir
# Concurrent agent performance:
defp test_performance_characteristics do
  # Spawn multiple agents and test system behavior under load
end
```

## Conclusion

Story 1.6's demo creation represents more than just a simple test scriptâ€”it's a comprehensive validation of Epic 1's architectural decisions and a template for demonstrating complex Elixir/OTP applications.

The demo successfully validates:
- **OTP Architecture**: Proper supervision tree implementation with fault tolerance
- **Agent Design**: Stateful GenServer patterns with external service integration
- **Provider Pattern**: Clean abstraction layer enabling model-agnostic LLM integration
- **Security Framework**: Sandboxed tool execution with comprehensive validation
- **Error Handling**: Graceful degradation with actionable user feedback

### Key Takeaways

1. **Integration Testing**: Demos serve as comprehensive integration tests for complex systems
2. **User Experience**: Clear, progressive output helps users understand system behavior
3. **Documentation**: Comprehensive troubleshooting guides reduce support burden
4. **Architecture Validation**: Runnable examples prove architectural decisions work in practice
5. **Educational Value**: Well-designed demos teach system architecture through practical examples

The demo pattern established in Story 1.6 will be extended throughout the remaining epics, providing a consistent way to validate and showcase The Maestro's evolving capabilities.

## Related Resources

- **[Agent Architecture Tutorial](../story1.3/)** - Understanding the GenServer patterns used in the demo
- **[LLM Provider Integration](../story1.4/)** - Deep dive into authentication and API integration
- **[Tooling System](../story1.5/)** - Security patterns and tool architecture
- **[Demo Code](../../../demos/epic1/)** - Complete runnable example with troubleshooting guide

---

*This tutorial demonstrates advanced Elixir/OTP patterns through practical, runnable examples that validate architectural decisions and provide educational value for intermediate to advanced developers.*